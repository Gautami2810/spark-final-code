spark {
   master = "local[*]"
   hadoop {
     fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
  }
  sql{
     parquet.fs.optimized.committer.optimization-enabled="true"
  }
}

# Input data configuration
input {
  file_path = "s3a://myawsinputbucket/input/emp_data.csv"
  format = "csv"
}

# Output data configuration
output {
  format = "parquet"
  file_path = "s3a://myawsinputbucket/output/partitioned_data_"
}
